/**
 * ===================================================================================================
 * useIntegratedVoiceSystem.js - í†µí•© ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œì„ ê´€ë¦¬í•˜ëŠ” ìµœìƒìœ„ React Hook
 * ===================================================================================================
 *
 * ğŸ“ ìœ„ì¹˜: í”„ë¡ íŠ¸ì—”ë“œ > client/src/hooks
 *
 * ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
 *    - ì—¬ëŸ¬ ìŒì„± ê´€ë ¨ í›…(`useBackgroundMonitoring`, `useVoiceCommands`, `useSharedAudioStream`, `useAudioManager`)ì„ í†µí•© ê´€ë¦¬
 *    - Web Speech API (`SpeechRecognition`)ì˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬ (ì´ˆê¸°í™”, ì‹œì‘, ì¬ì‹œì‘, ì¢…ë£Œ)
 *    - ìŒì„± ì¸ì‹ ê²°ê³¼(transcript)ë¥¼ ë¶„ì„í•˜ì—¬ 'ëª…ë ¹ì–´' ë˜ëŠ” 'ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§'ìœ¼ë¡œ ë¼ìš°íŒ…
 *    - í†µí•©ëœ ìŒì„± ìƒíƒœ(`voiceStatus`, `isAnalyzing`) ë° ë§ˆì´í¬ ë³¼ë¥¨(`micVolume`) ì œê³µ
 *
 * ğŸ”— ì—°ê²°ëœ íŒŒì¼:
 *    - src/App.js - ì•±ì˜ ìµœìƒìœ„ ì»´í¬ë„ŒíŠ¸ì—ì„œ ìŒì„± ì‹œìŠ¤í…œ ì „ì²´ë¥¼ ì œì–´í•˜ê¸° ìœ„í•´ ì‚¬ìš©
 *    - ./useBackgroundMonitoring.js - ë°±ê·¸ë¼ìš´ë“œ ì¼ì • ê°ì§€ í›…
 *    - ./useVoiceCommands.js - ìŒì„± ëª…ë ¹ì–´ ì²˜ë¦¬ í›…
 *    - ./useSharedAudioStream.js - ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ê³µìœ  í›…
 *    - ./useAudioManager.js - ë§ˆì´í¬ ë³¼ë¥¨ ë¶„ì„ í›…
 *
 * âœï¸ ìˆ˜ì • ê°€ì´ë“œ:
 *    - ìŒì„± ì¸ì‹ ì–¸ì–´ ë³€ê²½: `initializeRecognition` í•¨ìˆ˜ ë‚´ `recognition.lang` ê°’ì„ ìˆ˜ì •
 *    - ìŒì„± ê²°ê³¼ ì²˜ë¦¬ ë¡œì§ ë³€ê²½: `recognition.onresult` ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë‚´ë¶€ì˜ ë¡œì§ì„ ìˆ˜ì •í•˜ì—¬ ê²°ê³¼ ë¼ìš°íŒ… ë°©ì‹ì„ ë³€ê²½
 *    - ìŒì„± ì‹œìŠ¤í…œ í™œì„±í™” ì¡°ê±´ ë³€ê²½: `useEffect` ë‚´ì˜ `shouldBeRunning` ìƒìˆ˜ ë¡œì§ì„ ìˆ˜ì •
 *
 * ğŸ“ ì°¸ê³ ì‚¬í•­:
 *    - ì´ í›…ì€ ìŒì„± ê¸°ëŠ¥ì˜ í•µì‹¬ ì»¨íŠ¸ë¡¤ íƒ€ì›Œ ì—­í• ì„ í•©ë‹ˆë‹¤.
 *    - `isVoiceRecognitionEnabled` ìƒíƒœì— ë”°ë¼ ìŒì„± ê²°ê³¼ë¥¼ `useVoiceCommands`ë¡œ ë³´ë‚¼ì§€ ê²°ì •í•©ë‹ˆë‹¤.
 *    - ëª…ë ¹ì–´ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ìŒì„± ê²°ê³¼ëŠ” `isBackgroundMonitoring`ì´ í™œì„±í™”ëœ ê²½ìš° `useBackgroundMonitoring`ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
 *
 * ===================================================================================================
 */
import { useState, useEffect, useCallback, useRef } from 'react';
import { useBackgroundMonitoring } from './useBackgroundMonitoring';
import { useVoiceCommands } from './useVoiceCommands';
import { useSharedAudioStream } from './useSharedAudioStream';
import { useAudioManager } from './useAudioManager';

/**
 * useIntegratedVoiceSystem - ìŒì„± ì¸ì‹, ëª…ë ¹ì–´ ì²˜ë¦¬, ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” í›…
 *
 * @description ì´ í›…ì€ ì•±ì˜ ëª¨ë“  ìŒì„± ê´€ë ¨ ê¸°ëŠ¥ì„ ì´ê´„í•˜ë©°, `SpeechRecognition` APIë¥¼ ê´€ë¦¬í•˜ê³ 
 *              ìŒì„± ê²°ê³¼ë¥¼ ì ì ˆí•œ í•˜ìœ„ í›…(ëª…ë ¹ì–´ ì²˜ë¦¬ ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§)ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
 * @param {boolean} isLoggedIn - ì‚¬ìš©ì ë¡œê·¸ì¸ ì—¬ë¶€
 * @param {boolean} isVoiceRecognitionEnabled - ìŒì„± ëª…ë ¹ì–´ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
 * @param {object} eventActions - ì´ë²¤íŠ¸ ê´€ë ¨ í•¨ìˆ˜ë“¤ (ì¼ì • ì¶”ê°€ ë“±)
 * @param {boolean} areEventActionsReady - ì´ë²¤íŠ¸ ì•¡ì…˜ í•¨ìˆ˜ë“¤ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
 * @param {Function} setEventAddedKey - ì¼ì • ì¶”ê°€ ì‹œ UI ê°±ì‹ ì„ ìœ„í•œ í‚¤ ì„¤ì • í•¨ìˆ˜
 * @param {Function} handleChatMessage - ì±—ë´‡ ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜
 * @returns {object} í†µí•© ìŒì„± ì‹œìŠ¤í…œì˜ ìƒíƒœ ë° ì œì–´ í•¨ìˆ˜ë“¤ì„ í¬í•¨í•˜ëŠ” ê°ì²´
 * @property {boolean} isListening - `SpeechRecognition`ì´ í™œì„±í™”ë˜ì–´ ìŒì„±ì„ ë“£ê³  ìˆëŠ”ì§€ ì—¬ë¶€
 * @property {string} modalText - ìŒì„± ëª…ë ¹ì–´ ëª¨ë‹¬ì— í‘œì‹œë  í…ìŠ¤íŠ¸
 * @property {Function} setModalText - ìŒì„± ëª…ë ¹ì–´ ëª¨ë‹¬ í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
 * @property {boolean} isBackgroundMonitoring - ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ í™œì„±í™” ì—¬ë¶€
 * @property {string} voiceStatus - í˜„ì¬ í†µí•© ìŒì„± ì‹œìŠ¤í…œì˜ ìƒíƒœ ('waiting', 'recording', 'ending', 'analyzing', 'command')
 * @property {boolean} isAnalyzing - ë°±ê·¸ë¼ìš´ë“œ ë˜ëŠ” ëª…ë ¹ì–´ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì¸ì§€ ì—¬ë¶€
 * @property {number} micVolume - í˜„ì¬ ë§ˆì´í¬ ë³¼ë¥¨ (0.0 ~ 1.0)
 * @property {object|null} notification - ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  ì•Œë¦¼ ê°ì²´
 * @property {Function} clearNotification - ì•Œë¦¼ì„ ì§€ìš°ëŠ” í•¨ìˆ˜
 */
export const useIntegratedVoiceSystem = (
  isLoggedIn,
  isVoiceRecognitionEnabled,
  eventActions,
  areEventActionsReady,
  setEventAddedKey,
  handleChatMessage,
) => {
  const [isListening, setIsListening] = useState(false);
  const recognitionRef = useRef(null);
  const [isCommandProcessing, setIsCommandProcessing] = useState(false);

  const { getStream, stopStream } = useSharedAudioStream();
  const { micVolume } = useAudioManager();

  // ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ í›… ì‚¬ìš©
  const {
    isBackgroundMonitoring,
    processTranscript,
    voiceStatus: backgroundVoiceStatus, // ì´ë¦„ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë³€ê²½
    isAnalyzing: isBackgroundAnalyzing, // ì´ë¦„ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë³€ê²½
    notification,
    clearNotification,
    ...backgroundMonitoringProps
  } = useBackgroundMonitoring(eventActions, setEventAddedKey);

  // ìŒì„± ëª…ë ¹ì–´ ì²˜ë¦¬ ì‹œì‘/ì¢…ë£Œ í•¸ë“¤ëŸ¬
  const handleCommandStart = useCallback(() => setIsCommandProcessing(true), []);
  const handleCommandEnd = useCallback(() => setIsCommandProcessing(false), []);

  // ìŒì„± ëª…ë ¹ì–´ í›… ì‚¬ìš©
  const {
    modalText,
    setModalText,
    handleVoiceResult,
  } = useVoiceCommands(isLoggedIn, isVoiceRecognitionEnabled, handleChatMessage, {
    onCommandStart: handleCommandStart,
    onCommandEnd: handleCommandEnd,
  });

  const isMountedRef = useRef(true);

  // ì£¼ìš” ìƒíƒœ ë° í•¨ìˆ˜ë¥¼ refë¡œ ê´€ë¦¬í•˜ì—¬ onresult ì½œë°±ì—ì„œ ìµœì‹  ê°’ì„ ì°¸ì¡°í•˜ë„ë¡ í•¨
  const isBackgroundMonitoringRef = useRef(isBackgroundMonitoring);
  const isVoiceRecognitionEnabledRef = useRef(isVoiceRecognitionEnabled);
  const processTranscriptRef = useRef(processTranscript);
  const handleVoiceResultRef = useRef(handleVoiceResult);

  useEffect(() => { isBackgroundMonitoringRef.current = isBackgroundMonitoring; }, [isBackgroundMonitoring]);
  useEffect(() => { isVoiceRecognitionEnabledRef.current = isVoiceRecognitionEnabled; }, [isVoiceRecognitionEnabled]);
  useEffect(() => { processTranscriptRef.current = processTranscript; }, [processTranscript]);
  useEffect(() => { handleVoiceResultRef.current = handleVoiceResult; }, [handleVoiceResult]);

  // Web Speech API ì´ˆê¸°í™” ë° ì„¤ì •
  const initializeRecognition = useCallback(async () => {
    await getStream();

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      return;
    }

    recognitionRef.current = new SpeechRecognition();
    const recognition = recognitionRef.current;
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR';
    recognition.maxAlternatives = 1;

    recognition.onstart = () => setIsListening(true);

    // ìŒì„± ì¸ì‹ ê²°ê³¼ ì²˜ë¦¬
    recognition.onresult = event => {
      let currentTranscript = '';
      let isFinal = false;
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        currentTranscript += event.results[i][0].transcript;
        if (event.results[i].isFinal) isFinal = true;
      }
      
      let wasHandledByVoiceCommand = false;
      // ìŒì„± ëª…ë ¹ì–´ ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°, ìš°ì„ ì ìœ¼ë¡œ ëª…ë ¹ì–´ ì²˜ë¦¬
      if (isVoiceRecognitionEnabledRef.current) {
        wasHandledByVoiceCommand = handleVoiceResultRef.current(currentTranscript, isFinal, recognition);
      }

      // ëª…ë ¹ì–´ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ê³ , ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ì´ í™œì„±í™”ëœ ê²½ìš°, ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
      if (isBackgroundMonitoringRef.current && !wasHandledByVoiceCommand) {
        if (currentTranscript.trim()) {
          processTranscriptRef.current(currentTranscript, isFinal);
        }
      }
    };

    const restart = () => {
      if (recognitionRef.current && isMountedRef.current) {
        try {
          recognition.start();
        } catch (e) {
          // ì´ë¯¸ ì‹œì‘ ì¤‘ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—ëŸ¬ëŠ” ë¬´ì‹œ
        }
      }
    }

    recognition.onerror = event => {
      if (event.error !== 'no-speech' && event.error !== 'aborted') {
        // 'no-speech'ë‚˜ 'aborted' ì™¸ì˜ ì—ëŸ¬ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
      }
    };

    // ì¸ì‹ ì„¸ì…˜ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ì¬ì‹œì‘
    recognition.onend = () => {
      if (isMountedRef.current && recognitionRef.current) {
        restart();
      }
    };

    try {
      recognition.start();
    } catch (error) {
      // ì‹œì‘ ì—ëŸ¬ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
    }
  }, [getStream]);

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸/ì–¸ë§ˆìš´íŠ¸ ë° ì¡°ê±´ì— ë”°ë¼ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ ìƒëª…ì£¼ê¸° ê´€ë¦¬
  useEffect(() => {
    isMountedRef.current = true;
    const shouldBeRunning = isLoggedIn && areEventActionsReady && (isVoiceRecognitionEnabled || isBackgroundMonitoring);

    if (shouldBeRunning) {
      if (!recognitionRef.current) initializeRecognition();
    } else {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
    }
    return () => { isMountedRef.current = false; };
  }, [isLoggedIn, areEventActionsReady, isVoiceRecognitionEnabled, isBackgroundMonitoring, initializeRecognition]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
        recognitionRef.current = null;
      }
      stopStream();
    };
  }, [stopStream]);

  // UIì— í‘œì‹œë  ìµœì¢… ìŒì„± ìƒíƒœ ê²°ì • (ë°±ê·¸ë¼ìš´ë“œ ìƒíƒœê°€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§)
  const voiceStatus = backgroundVoiceStatus !== 'waiting' ? backgroundVoiceStatus : (isCommandProcessing ? 'command' : 'waiting');
  const isAnalyzing = isBackgroundAnalyzing || isCommandProcessing;

  return { 
    isListening, 
    modalText,
    setModalText,
    isBackgroundMonitoring,
    voiceStatus,
    isAnalyzing,
    micVolume,
    notification,
    clearNotification,
    ...backgroundMonitoringProps
  };
};
